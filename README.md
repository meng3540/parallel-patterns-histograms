
# Histograms

#### Implementing and optimizing a CUDA-based histogram algorithm

## What is a Histogram?

A histogram is a graphical representation of the distribution of data. It displays the frequency or occurrence of values within specified intervals or bins. On a typical histogram, the horizontal axis is divided up into groups or bins (eg. a-d, e-h, i-l, etc.) and the number of occurrences of values in the data that are within a particular bin is graphed as a bar on the vertical axis. An example of a histogram is pictured below:

![Histogram of R](/res/Example_histogram.png "By Visnut - Using R from simulated data, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=36192473")

### Summary of Histograms and CUDA

As a basis for this assignment a summary of histograms, and how they may be accelerated by using NVIDIA's CUDA platform, was generated using Chat-GPT 3.5\. The output generated by Chat-GPT may be viewed in the [documentation in the docs folder](/doc/Histograms_Summary.md).

## Basic algorithm

The algorithm for the basic implementation of a histogram in CUDA was provided by the course textbook: [Programming Massively Parallel Processors, 3rd Edition by Morgan Kaufmann](https://learning.oreilly.com/library/view/programming-massively-parallel/9780128119877/).

```C++
__global__ void calculateHisto(char* buffer, int* histo, int size, int numBins)
{
    int i = threadIdx.x + blockIdx.x * blockDim.x;
    int sectSize = (size - 1) / (blockDim.x * gridDim.x) + 1;
    int start = i * sectSize;

    for (int k = 0; k < sectSize; k++) {
        if (start + k < size) {
            int alphaPos = buffer[start + k] - 'a';
            if (alphaPos >= 0 && alphaPos < numBins) {
                atomicAdd(&(histo[alphaPos]), 1);
            }
        }
    }
}
```

The whole code can be found [under the src folder](/src/No_Optimizations/kernel_noOptim.cu).

In this implementation, the function takes in an array (the string of characters to take the histogram of), an output array that represents the histogram, a size parameter defining the size of the input array, and takes in the number of bins to sort into. **It should be noted that the basic algorithm does not account for binning multiple letters together, and only returns valid results for 26 bins; one per letter.**

The code defines some variables, namely i (the index for the current thread), sectSize (to split up the input array so each thread handles the right number of input elements so that all elements get processed) and a start variable (effectively the start offset in the input array for each thread to begin at).

```C++
int i = threadIdx.x + blockIdx.x * blockDim.x;
int sectSize = (size - 1) / (blockDim.x * gridDim.x) + 1;
int start = i * sectSize;
```

The function then iterates over the section assigned to the thread and if the current loop will still be processing input data (as in if the start position plus the current K value is still referring to an array index within the input array; there is valid work for this thread to do), then we calculate the numerical position of the character in the alphabet. This is achieved by subtracting "a" from the value in the input array. If the letter is a the result is 0, if the letter is b then the result is 1, and so on. If this value is a valid alphabet letter (from 0 to the number of bins), the count in the corresponding bin is incremented by 1\. As the computations are happening in parallel, it is possible that two threads may try to increment the same bin at the same time. This would cause an issue (race condition) and therefore the atomicAdd() function is used to ensure only one thread can perform an action on an address (bin) at a time.

```C++
for (int k = 0; k < sectSize; k++) {
        if (start + k < size) {
            int alphaPos = buffer[start + k] - 'a';
            if (alphaPos >= 0 && alphaPos < numBins) {
                atomicAdd(&(histo[alphaPos]), 1);
            }
        }
    }
```

### Metrics: Timing Execution Time

To provide an easy metric to compare with, the code has an event timer added through the use of cudaEvent functions. The general form for adding these is as follows:

```C++
cudaEvent_t start, stop;
cudaEventCreate(&start);
cudaEventCreate(&stop);

 //Kernel Launch
cudaEventRecord(start, 0);
kernel << < gridDim, blockDim>> > ();
cudaEventRecord(stop, 0);
cudaEventSynchronize(stop);
float milliseconds = 0;
cudaEventElapsedTime(&milliseconds, start, stop);

printf("\n");
printf("The kernel took %.2f milliseconds to execute.\n", milliseconds);
```

### Outputs

The results of running the un-optimized kernel are depicted below:

![Histogram of enwik8](/res/NoOptim_Time.png "Kernel Runtime without optimization: 30.54ms")

## Optimizations

### Block Size Optimization

The block size usually has a major impact on performance for parallel processes. Launching too many or too few threads per block can lead to issues with occupancy (the ratio of the number of active warps to the maximum supported number) as well as other issues such as exhausting limited resources like registers and shared memory. Our code first used 32 threads per block. Using the occupancy calculator in Nsight, we can see that the optimum number of threads per block suggested by NSight is between 80-128, however we will choose 128 as it is a multiple of 32 (the warp size).

![Occupancy Calculator](/res/NoOptim_OccCalc.png "Suggested thread count for maximum occupancy")

After setting out thread count to 128, we can see there is a bit of an improvement, however we other optimizations to implement for further improvement.

![Histogram of enwik8](/res/NoOptim_Time128.png "Kernel Runtime without optimization: 30.54ms")

### Interleaved Partitioning

Our un-optimized kernel partitioned the input data into chunks and assigned one chunk per thread. This is not memory efficient as each thread has to make a memory call to get its data as the data is non-consecutive. To improve this, we can restructure the code to assign the available threads consecutive data from the input. This means only one memory access is required to load the data needed by the threads, resulting in fewer memory accesses and better throughput.

The modified kernel is as follows:
```C++
__global__ void calculateHisto(char* buffer, int* histo, int size, int numBins)
{
    int i = threadIdx.x + blockIdx.x * blockDim.x;

    extern __shared__ unsigned int histos[];
    for (int binIdx = threadIdx.x; binIdx < numBins; binIdx += blockDim.x) {
        histos[binIdx] = 0u;
    }
    __syncthreads();

    for (int k = i; k < size; k+=blockDim.x*gridDim.x) {
        int alphaPos = buffer[k] - 'a';
        if (alphaPos >= 0 && alphaPos < numBins) {
            atomicAdd(&(histos[alphaPos]), 1);
        }
    }

    __syncthreads();
    for (int binIdx = threadIdx.x; binIdx < numBins; binIdx += blockDim.x) {
        atomicAdd(&(histo[binIdx]),histos[binIdx]);
    }
}
```


### Privatization Optimization
Currently, all the operations in our algorithm are reading and writing to global memory. This is perhaps the easiest memory to work with as it is large and accessible by every thread, however it is very slow and leads to a lot of memory access latency. To increase the throughput of our atomic operations, utilizing the shared memory would be much better. This memory is smaller, but many times faster than the global memory. By using shared memory, a new problem will also emerge, only threads in the same block can access the same shared memory. As our code will launch multiple blocks, all our atomic operations cannot store their results in the same location and we will need to work around this.

First, however, we will implement the prioritized memory for each thread. This is relatively simple, as we can define a temporary histogram array in the shared memory

```C++
extern __shared__ unsigned int histos[];
for (int binIdx = threadIdx.x; binIdx < numBins; binIdx += blockDim.x) {
    histos[binIdx] = 0u;
}
__syncthreads();
```

We can then generate our histogram as before. Once all the threads in the block finish, we simply need to add the privatized histograms from each block to the overall histogram result.

```C++
__syncthreads();
for (int binIdx = threadIdx.x; binIdx < numBins; binIdx += blockDim.x) {
    atomicAdd(&(histo[binIdx]),histos[binIdx]);
}
```


###### A very small portion of this document contains content written by Chat-GPT. All information has been reviewed and deemed accurate by the authors.